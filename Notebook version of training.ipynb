{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils \n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-do',  help='Dropout param [default: 0.5]')\n",
    "    parser.add_argument('-a',   help='Conv Layers LeakyReLU alpha param [if alpha set to 0 LeakyReLU is equivalent with ReLU] [default: 0.3]')\n",
    "    parser.add_argument('-k',   help='Feature maps k multiplier [default: 4]')\n",
    "    parser.add_argument('-cl',  help='Number of Convolutional Layers [default: 5]')\n",
    "    parser.add_argument('-s',   help='Input Image rescale factor [default: 1]')\n",
    "    parser.add_argument('-pf',  help='Percentage of the pooling layer: [0,1] [default: 1]')\n",
    "    parser.add_argument('-pt',  help='Pooling type: \\'Avg\\', \\'Max\\' [default: Avg]')\n",
    "    parser.add_argument('-fp',  help='Feature maps policy: \\'proportional\\',\\'static\\' [default: proportional]')\n",
    "    parser.add_argument('-opt', help='Optimizer: \\'SGD\\',\\'Adagrad\\',\\'Adam\\' [default: Adam]')\n",
    "    parser.add_argument('-obj', help='Minimization Objective: \\'mse\\',\\'ce\\' [default: ce]')\n",
    "    parser.add_argument('-pat', help='Patience parameter for early stoping [default: 200]')\n",
    "    parser.add_argument('-tol', help='Tolerance parameter for early stoping [default: 1.005]')\n",
    "    parser.add_argument('-csv', help='csv results filename alias [default: res]')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # loading mnist dataset\n",
    "    (X_train, y_train), (X_val, y_val) = mnist.load_data()\n",
    "\n",
    "    # adding a singleton dimension and rescale to [0,1]\n",
    "    X_train = np.asarray(np.expand_dims(X_train,1))/float(255)\n",
    "    X_val = np.asarray(np.expand_dims(X_val,1))/float(255)\n",
    "\n",
    "    # labels to categorical vectors\n",
    "    uniquelbls = np.unique(y_train)\n",
    "    nb_classes = uniquelbls.shape[0]\n",
    "    zbn = np.min(uniquelbls) # zero based numbering\n",
    "    y_train = np_utils.to_categorical(y_train-zbn, nb_classes)\n",
    "    y_val = np_utils.to_categorical(y_val-zbn, nb_classes)\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(actual,pred):\n",
    "    fscore = metrics.f1_score(actual, pred, average='macro')\n",
    "    acc = metrics.accuracy_score(actual, pred)\n",
    "    cm = metrics.confusion_matrix(actual,pred)\n",
    "\n",
    "    return fscore, acc, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import helpers as H\n",
    "# import cnn_model as CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args         = parse_args()                          # Function for parcing command-line arguments\n",
    "train_params = {\n",
    "     'do' : float(args.do) if args.do else 0.5,        # Dropout Parameter\n",
    "     'a'  : float(args.a) if args.a else 0.3,          # Conv Layers LeakyReLU alpha param [if alpha set to 0 LeakyReLU is equivalent with ReLU]\n",
    "     'k'  : int(args.k) if args.k else 4,              # Feature maps k multiplier\n",
    "     's'  : float(args.s) if args.s else 1,            # Input Image rescale factor\n",
    "     'pf' : float(args.pf) if args.pf else 1,          # Percentage of the pooling layer: [0,1]\n",
    "     'pt' : args.pt if args.pt else 'Avg',             # Pooling type: Avg, Max\n",
    "     'fp' : args.fp if args.fp else 'proportional',    # Feature maps policy: proportional, static\n",
    "     'cl' : int(args.cl) if args.cl else 5,            # Number of Convolutional Layers\n",
    "     'opt': args.opt if args.opt else 'Adam',          # Optimizer: SGD, Adagrad, Adam\n",
    "     'obj': args.obj if args.obj else 'ce',            # Minimization Objective: mse, ce\n",
    "     'patience' : args.pat if args.pat else 200,       # Patience parameter for early stoping\n",
    "     'tolerance': args.tol if args.tol else 1.005,     # Tolerance parameter for early stoping [default: 1.005, checks if > 0.5%]\n",
    "     'res_alias': args.csv if args.csv else 'res'      # csv results filename alias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading mnist data as example\n",
    "(X_train, y_train), (X_val, y_val) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D,AveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_FeatureMaps(L, policy, constant=17):\n",
    "    return {\n",
    "        'proportional': (L+1)**2,\n",
    "        'static': constant,\n",
    "    }[policy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Obj(obj):\n",
    "    return {\n",
    "        'mse': 'MSE',\n",
    "        'ce': 'categorical_crossentropy',\n",
    "    }[obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.3,\n",
       " 'cl': 5,\n",
       " 'do': 0.5,\n",
       " 'fp': '/Users/peterhirt/Library/Jupyter/runtime/kernel-972781be-088a-4622-a1a6-de11e62d0959.json',\n",
       " 'k': 4,\n",
       " 'obj': 'ce',\n",
       " 'opt': 'Adam',\n",
       " 'patience': 200,\n",
       " 'pf': 1,\n",
       " 'pt': 'Avg',\n",
       " 'res_alias': 'res',\n",
       " 's': 1,\n",
       " 'tolerance': 1.005}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x_train, y_train, x_val, y_val, params):\n",
    "    ''' TODO: documentation '''\n",
    "\n",
    "    \n",
    "    # Parameters String used for saving the files\n",
    "    parameters_str = str('_d' + str(params['do']).replace('.', '') +\n",
    "                         '_a' + str(params['a']).replace('.', '') + \n",
    "                         '_k' + str(params['k']).replace('.', '') + \n",
    "                         '_c' + str(params['cl']).replace('.', '') + \n",
    "                         '_s' + str(params['s']).replace('.', '') + \n",
    "                         '_pf' + str(params['pf']).replace('.', '') + \n",
    "                         '_pt' + params['pt'] +\n",
    "                         '_fp' + str(params['fp']).replace('.', '') +\n",
    "                         '_opt' + params['opt'] +\n",
    "                         '_obj' + params['obj'])\n",
    "\n",
    "    # Printing the parameters of the model\n",
    "    print('[Dropout Param] \\t->\\t'+str(params['do']))\n",
    "    print('[Alpha Param] \\t\\t->\\t'+str(params['a']))\n",
    "    print('[Multiplier] \\t\\t->\\t'+str(params['k']))\n",
    "    print('[Patience] \\t\\t->\\t'+str(params['patience']))\n",
    "    print('[Tolerance] \\t\\t->\\t'+str(params['tolerance']))\n",
    "    print('[Input Scale Factor] \\t->\\t'+str(params['s']))\n",
    "    print('[Pooling Type] \\t\\t->\\t'+ params['pt'])\n",
    "    print('[Pooling Factor] \\t->\\t'+str(str(params['pf']*100)+'%'))\n",
    "    print('[Feature Maps Policy] \\t->\\t'+ params['fp'])\n",
    "    print('[Optimizer] \\t\\t->\\t'+ params['opt'])\n",
    "    print('[Objective] \\t\\t->\\t'+ get_Obj(params['obj']))\n",
    "    print('[Results filename] \\t->\\t'+str(params['res_alias']+parameters_str+'.txt'))\n",
    "\n",
    "    # Rescale Input Images\n",
    "    if params['s'] != 1:\n",
    "        print('\\033[93m'+'Rescaling Patches...'+'\\033[0m')\n",
    "        x_train = np.asarray(np.expand_dims([cv2.resize(x_train[i, 0, :, :], (0,0), fx=params['s'], fy=params['s']) for i in xrange(x_train.shape[0])], 1))\n",
    "        x_val = np.asarray(np.expand_dims([cv2.resize(x_val[i, 0, :, :], (0,0), fx=params['s'], fy=params['s']) for i in xrange(x_val.shape[0])], 1))\n",
    "        print('\\033[92m'+'Done, Rescaling Patches'+'\\033[0m')\n",
    "        print('[New Data Shape]\\t->\\tX: '+str(x_train.shape))\n",
    "\n",
    "    model = get_model(x_train.shape, y_train.shape, params)\n",
    "\n",
    "    # Counters-buffers\n",
    "    maxf         = 0\n",
    "    maxacc       = 0\n",
    "    maxit        = 0\n",
    "    maxtrainloss = 0\n",
    "    maxvaloss    = np.inf\n",
    "    p            = 0\n",
    "    it           = 0\n",
    "    best_model   = model\n",
    "\n",
    "    # Open file to write the results\n",
    "    open(params['res_alias']+parameters_str+'.csv', 'a').write('Epoch, Val_fscore, Val_acc, Train_loss, Val_loss\\n')\n",
    "    open(params['res_alias']+parameters_str+'-Best.csv', 'a').write('Epoch, Val_fscore, Val_acc, Train_loss, Val_loss\\n')\n",
    "    \n",
    "    while p < params['patience']:\n",
    "        p += 1\n",
    "\n",
    "        # Fit the model for one epoch\n",
    "        print('Epoch: ' + str(it))\n",
    "        history = model.fit(x_train, y_train, batch_size=128, nb_epoch=1, validation_data=(x_val,y_val), shuffle=True)\n",
    "\n",
    "        # Evaluate models\n",
    "        y_score = model.predict(x_val, batch_size=1050)\n",
    "        fscore, acc, cm = H.evaluate(np.argmax(y_val, axis=1), np.argmax(y_score, axis=1))\n",
    "        print('Val F-score: '+str(fscore)+'\\tVal acc: '+str(acc))\n",
    "\n",
    "        # Write results in file\n",
    "        open(params['res_alias']+parameters_str+'.csv', 'a').write(str(str(it)+', '+str(fscore)+', '+str(acc)+', '+str(np.max(history.history['loss']))+', '+str(np.max(history.history['val_loss']))+'\\n'))\n",
    "\n",
    "        # check if current state of the model is the best and write evaluation metrics to file\n",
    "        if fscore > maxf*params['tolerance']:  # if fscore > maxf*params['tolerance']:\n",
    "            p            = 0  # restore patience counter\n",
    "            best_model   = model  # store current model state\n",
    "            maxf         = fscore \n",
    "            maxacc       = acc\n",
    "            maxit        = it\n",
    "            maxtrainloss = np.max(history.history['loss'])\n",
    "            maxvaloss    = np.max(history.history['val_loss'])\n",
    "\n",
    "            print(np.round(100*cm/np.sum(cm,axis=1).astype(float)))\n",
    "            open(params['res_alias']+parameters_str+'-Best.csv', 'a').write(str(str(maxit)+', '+str(maxf)+', '+str(maxacc)+', '+str(maxtrainloss)+', '+str(maxvaloss)+'\\n'))\n",
    "\n",
    "        it += 1\n",
    "\n",
    "    print('Max: fscore:', maxf, 'acc:', maxacc, 'epoch: ', maxit, 'train loss: ', maxtrainloss, 'validation loss: ', maxvaloss)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_shape, output_shape, params):\n",
    "\n",
    "    print('compiling model...')\n",
    "        \n",
    "    # Dimension of The last Convolutional Feature Map (eg. if input 32x32 and there are 5 conv layers 2x2 fm_size = 27)\n",
    "    fm_size = input_shape[-1] - params['cl']\n",
    "    \n",
    "    # Tuple with the pooling size for the last convolutional layer using the params['pf']\n",
    "    pool_siz = (np.round(fm_size*params['pf']).astype(int), np.round(fm_size*params['pf']).astype(int))\n",
    "    \n",
    "    # Initialization of the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # keras.layers.convolutional.Convolution2D(\n",
    "    # nb_filter, : Number of convolution filters to use.\n",
    "    # nb_row,     : Number of rows in the convolution kernel.\n",
    "    # nb_col,     : Number of columns in the convolution kernel. \n",
    "    # init='glorot_uniform',  : name of initialization function for the weights of the layer \n",
    "    #                          (see initializations), or alternatively, Theano function to use for weights initialization. \n",
    "    #                           This parameter is only relevant if you don't pass a weights argument.\n",
    "    # activation='linear',     : name of activation function to use (see activations), or alternatively, elementwise Theano function. \n",
    "    #                            If you don't specify anything, no activation is applied \n",
    "    # weights=None,            : list of numpy arrays to set as initial weights.\n",
    "    # border_mode='valid',     : 'valid' or 'same'.\n",
    "    # subsample=(1, 1),        : tuple of length 2. Factor by which to subsample output. Also called strides elsewhere.\n",
    "    # dim_ordering='th',    : 'th' or 'tf'. In 'th' mode, the channels dimension (the depth) is at index 1, in 'tf' mode is it at index 3.\n",
    "    # W_regularizer=None,    : : instance of WeightRegularizer (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
    "    # b_regularizer=None,    : instance of WeightRegularizer, applied to the bias.\n",
    "    # activity_regularizer=None,  : instance of ActivityRegularizer, applied to the network output.\n",
    "    # W_constraint=None,          : instance of the constraints module (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
    "    # b_constraint=None)          : instance of the constraints module, applied to the bias.\n",
    "    \n",
    "    # params['fp'] = {\n",
    "    # \"stdin_port\": 55807, \n",
    "    # \"ip\": \"127.0.0.1\", \n",
    "    # \"control_port\": 55808, \n",
    "    # \"hb_port\": 55809, \n",
    "    # \"signature_scheme\": \"hmac-sha256\", \n",
    "    # \"key\": \"1fe89813-e1e0-4524-873b-eb2d9e6882d7\", \n",
    "    # \"kernel_name\": \"\", \n",
    "    # \"shell_port\": 55805, \n",
    "    # \"transport\": \"tcp\", \n",
    "    # \"iopub_port\": 55806\n",
    "    # }\n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(16, 2, 2, init='orthogonal', activation=LeakyReLU(0.3), \n",
    "                            input_shape=input_shape[1:]))\n",
    "    \n",
    "    model.add(AveragePooling2D(pool_size=pool_siz))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(params['do']))\n",
    "    \n",
    "    model.add(Dense(int(params['k']*get_FeatureMaps(params['cl'], params['fp']))/params['pf']*6, init='he_uniform', activation=LeakyReLU(0)))\n",
    "    model.add(Dropout(params['do']))\n",
    "    model.add(Dense(int(params['k']*get_FeatureMaps(params['cl'], params['fp']))/params['pf']*2, init='he_uniform', activation=LeakyReLU(0)))\n",
    "    model.add(Dropout(params['do']))\n",
    "    model.add(Dense(output_shape[1], init='he_uniform', activation='softmax'))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train1(x_train, y_train, x_val, y_val, params):\n",
    "    ''' TODO: documentation '''\n",
    "\n",
    "    \n",
    "    # Parameters String used for saving the files\n",
    "    parameters_str = str('_d' + str(params['do']).replace('.', '') +\n",
    "                         '_a' + str(params['a']).replace('.', '') + \n",
    "                         '_k' + str(params['k']).replace('.', '') + \n",
    "                         '_c' + str(params['cl']).replace('.', '') + \n",
    "                         '_s' + str(params['s']).replace('.', '') + \n",
    "                         '_pf' + str(params['pf']).replace('.', '') + \n",
    "                         '_pt' + params['pt'] +\n",
    "                         '_fp' + str(params['fp']).replace('.', '') +\n",
    "                         '_opt' + params['opt'] +\n",
    "                         '_obj' + params['obj'])\n",
    "\n",
    "    # Printing the parameters of the model\n",
    "    print('[Dropout Param] \\t->\\t'+str(params['do']))\n",
    "    print('[Alpha Param] \\t\\t->\\t'+str(params['a']))\n",
    "    print('[Multiplier] \\t\\t->\\t'+str(params['k']))\n",
    "    print('[Patience] \\t\\t->\\t'+str(params['patience']))\n",
    "    print('[Tolerance] \\t\\t->\\t'+str(params['tolerance']))\n",
    "    print('[Input Scale Factor] \\t->\\t'+str(params['s']))\n",
    "    print('[Pooling Type] \\t\\t->\\t'+ params['pt'])\n",
    "    print('[Pooling Factor] \\t->\\t'+str(str(params['pf']*100)+'%'))\n",
    "    print('[Feature Maps Policy] \\t->\\t'+ params['fp'])\n",
    "    print('[Optimizer] \\t\\t->\\t'+ params['opt'])\n",
    "    print('[Objective] \\t\\t->\\t'+ get_Obj(params['obj']))\n",
    "    print('[Results filename] \\t->\\t'+str(params['res_alias']+parameters_str+'.txt'))\n",
    "    \n",
    "    # Rescale Input Images\n",
    "    if params['s'] != 1:\n",
    "        print('\\033[93m'+'Rescaling Patches...'+'\\033[0m')\n",
    "        x_train = np.asarray(np.expand_dims([cv2.resize(x_train[i, 0, :, :], (0,0), fx=params['s'], fy=params['s']) for i in xrange(x_train.shape[0])], 1))\n",
    "        x_val = np.asarray(np.expand_dims([cv2.resize(x_val[i, 0, :, :], (0,0), fx=params['s'], fy=params['s']) for i in xrange(x_val.shape[0])], 1))\n",
    "        print('\\033[92m'+'Done, Rescaling Patches'+'\\033[0m')\n",
    "        print('[New Data Shape]\\t->\\tX: '+str(x_train.shape))\n",
    "        \n",
    "    model = get_model(x_train.shape, y_train.shape, params)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dropout Param] \t->\t0.5\n",
      "[Alpha Param] \t\t->\t0.3\n",
      "[Multiplier] \t\t->\t4\n",
      "[Patience] \t\t->\t200\n",
      "[Tolerance] \t\t->\t1.005\n",
      "[Input Scale Factor] \t->\t1\n",
      "[Pooling Type] \t\t->\tAvg\n",
      "[Pooling Factor] \t->\t100%\n",
      "[Feature Maps Policy] \t->\t/Users/peterhirt/Library/Jupyter/runtime/kernel-972781be-088a-4622-a1a6-de11e62d0959.json\n",
      "[Optimizer] \t\t->\tAdam\n",
      "[Objective] \t\t->\tcategorical_crossentropy\n",
      "[Results filename] \t->\tres_d05_a03_k4_c5_s1_pf1_ptAvg_fp/Users/peterhirt/Library/Jupyter/runtime/kernel-972781be-088a-4622-a1a6-de11e62d0959json_optAdam_objce.txt\n",
      "compiling model...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'/Users/peterhirt/Library/Jupyter/runtime/kernel-972781be-088a-4622-a1a6-de11e62d0959.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ad121c20eb38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-d2e62f61171c>\u001b[0m in \u001b[0;36mtrain1\u001b[0;34m(x_train, y_train, x_val, y_val, params)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[New Data Shape]\\t->\\tX: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-cd447827484a>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(input_shape, output_shape, params)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'do'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mget_FeatureMaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'he_uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'do'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mget_FeatureMaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'he_uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8c8dc4cf95de>\u001b[0m in \u001b[0;36mget_FeatureMaps\u001b[0;34m(L, policy, constant)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m'proportional'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'static'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     }[policy]\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '/Users/peterhirt/Library/Jupyter/runtime/kernel-972781be-088a-4622-a1a6-de11e62d0959.json'"
     ]
    }
   ],
   "source": [
    "model = train1(X_train, y_train, X_val, y_val, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
