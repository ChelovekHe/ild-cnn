{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.0rc0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pkg_resources as pkg_rs\n",
    "tensorflow_version = pkg_rs.get_distribution(\"tensorflow\").version\n",
    "print tensorflow_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('summaries_dir', '../tensorboard1', 'Summaries directory')\n",
    "\n",
    "# launch tensorboard with\n",
    "# tensorboard --logdir='/Users/peterhirt/datascience/ild-cnn/tensorboard1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the tensorboard data directory exists, delete the content and create an empty directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if tf.gfile.Exists(FLAGS.summaries_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.summaries_dir)\n",
    "tf.gfile.MakeDirs(FLAGS.summaries_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load ILD_CNN dataset\n",
    "\n",
    "both training set and validation set is loaded, normalised and the labels hot-encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    # load the dataset as X_train and as a copy the X_val\n",
    "    # the labels are loaded as one-hot encoded data\n",
    "    \n",
    "    X_train = pickle.load( open( \"../pickle/X_train.pkl\", \"rb\" ) )\n",
    "    y_train = pickle.load( open( \"../pickle/y_train_hot.pkl\", \"rb\" ) )\n",
    "    X_val = pickle.load( open( \"../pickle/X_val.pkl\", \"rb\" ) )\n",
    "    y_val = pickle.load( open( \"../pickle/y_val_hot.pkl\", \"rb\" ) )\n",
    "    \n",
    "    X_test = pickle.load( open( \"../pickle/X_test.pkl\", \"rb\" ) )\n",
    "    y_test = pickle.load( open( \"../pickle/y_test_hot.pkl\", \"rb\" ) )\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # adding a singleton dimension and rescale to [0,1]\n",
    "    X_train = np.asarray(np.expand_dims(X_train, axis=3))/float(255)\n",
    "    X_val = np.asarray(np.expand_dims(X_val,axis=3))/float(255)\n",
    "    X_test = np.asarray(np.expand_dims(X_test,axis=3))/float(255)\n",
    "    \n",
    "    X_train = np.float32(X_train)\n",
    "    X_val = np.float32(X_val)\n",
    "    X_test = np.float32(X_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 32, 32, 1)\n",
      "float32\n",
      "(12500, 6)\n",
      "float32\n",
      "(6250, 32, 32, 1)\n",
      "(6250, 6)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "# the labels are one-hot-encoded\n",
    "\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 32, 32, 1)\n",
      "(12500, 6)\n",
      "type of training set:  float32\n",
      "shape of test set\n",
      "(6250, 32, 32, 1)\n",
      "(6250, 6)\n"
     ]
    }
   ],
   "source": [
    "# shape and type of the training set\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print 'type of training set: ', X_train.dtype \n",
    "\n",
    "print 'shape of test set'\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model definitions\n",
    "    batch size : 128\n",
    "    interations : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 10000\n",
    "batch_size = 128\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "\n",
    "dropout = 0.5 # Dropout, probability to keep units\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# tf Graph input\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 32, 32, 1], name='x-input')\n",
    "    y = tf.placeholder(tf.float32, [None, 6], name='y-input')\n",
    "    \n",
    "\n",
    "# dropout (keep probability)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size):\n",
    "    \n",
    "    n_samples = len(X_train)\n",
    "    indices = np.random.choice(n_samples, batch_size)\n",
    "    batch_x = X_train[indices]\n",
    "    batch_y = y_train[indices]\n",
    "    \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_flatten(x):\n",
    "    # Turn a n-D tensor into a 2D tensor where the first dimension is conserved.\n",
    "    \n",
    "    x = tf.reshape(x, [-1, np.prod(x.get_shape()[1:].as_list())])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):\n",
    "    \n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.scalar_summary('mean/' + name, mean)\n",
    "    stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "    \n",
    "    tf.scalar_summary('sttdev/' + name, stddev)\n",
    "    tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "    tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "    tf.histogram_summary(name, var)\n",
    "    \n",
    "def conv2d(x, W, b, convLayer, strides=1):\n",
    "    with tf.name_scope(convLayer):\n",
    "        # Conv2D wrapper, with bias and relu activation\n",
    "        x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME', name=convLayer)\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        #variable_summaries(W, convLayer + '/weights')\n",
    "        #variable_summaries(b, convLayer + '/biases')\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k):\n",
    "    with tf.name_scope('AvgPooling'):\n",
    "        # MaxPool2D wrapper\n",
    "        return tf.nn.avg_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "with tf.name_scope('weights'):\n",
    "    weights = {\n",
    "        # 2x2 conv, 1 input, 16 outputs\n",
    "        'wc1': tf.Variable(tf.random_normal([2, 2, 1, 16])),\n",
    "        # 2x2 conv, 16 inputs, 36 outputs\n",
    "        'wc2': tf.Variable(tf.random_normal([2, 2, 16, 36])),\n",
    "        # 2x2 conv, 36 inputs, 64 outputs\n",
    "        'wc3': tf.Variable(tf.random_normal([2, 2, 36, 64])),\n",
    "        # 2x2 conv, 64 inputs, 100 outputs\n",
    "        'wc4': tf.Variable(tf.random_normal([2, 2, 64, 100])),\n",
    "        # 2x2 conv, 100 inputs, 144 outputs\n",
    "        'wc5': tf.Variable(tf.random_normal([2, 2, 100, 144])),\n",
    "        \n",
    "        # fully connected, 144 inputs, 864 outputs\n",
    "        'wd1': tf.Variable(tf.random_normal([144, 864])),\n",
    "        # fully connected, 144 inputs, 864 outputs\n",
    "        'wd2': tf.Variable(tf.random_normal([864, 288])),\n",
    "        # 1024 inputs, 10 outputs (class prediction)\n",
    "        'out': tf.Variable(tf.random_normal([288, 6]))\n",
    "        }\n",
    "\n",
    "with tf.name_scope('bias'):\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.random_normal([16])),\n",
    "        'bc2': tf.Variable(tf.random_normal([36])),\n",
    "        'bc3': tf.Variable(tf.random_normal([64])),\n",
    "        'bc4': tf.Variable(tf.random_normal([100])),\n",
    "        'bc5': tf.Variable(tf.random_normal([144])),\n",
    "    \n",
    "        'bd1': tf.Variable(tf.random_normal([864])),\n",
    "        'bd2': tf.Variable(tf.random_normal([288])),\n",
    "        'out': tf.Variable(tf.random_normal([6]))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "def conv_net(x, weights, biases):\n",
    "    \n",
    "    # Convolution Layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'], 'convolution2D_1')\n",
    "    \n",
    "    # Convolution Layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'], 'convolution2D_2')\n",
    "    \n",
    "    # Convolution Layer 3\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'], 'convolution2D_3')\n",
    "    \n",
    "    # Convolution Layer 4\n",
    "    conv4 = conv2d(conv3, weights['wc4'], biases['bc4'], 'convolution2D_4')\n",
    "    \n",
    "    # Convolution Layer 5\n",
    "    conv5 = conv2d(conv4, weights['wc5'], biases['bc5'], 'convolution2D_5')\n",
    "    \n",
    "    pool1 = maxpool2d(conv5, k=32)\n",
    "    \n",
    "    # must do flattening and dropout first\n",
    "    #pool1_reshaped = tf.squeeze(pool1, [1, 2])\n",
    "    pool1_reshaped = batch_flatten(pool1)\n",
    "    pool1_dropout = tf.nn.dropout(pool1_reshaped, keep_prob)\n",
    "    \n",
    "    with tf.name_scope('dense1'):\n",
    "        \n",
    "        # pool1 should be 144 rank 1 tensor, weights should be 864x144, bias should be 864\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(pool1_dropout, weights['wd1']) + biases['bd1'])\n",
    "        \n",
    "        # again dropout op needed\n",
    "        \n",
    "    with tf.name_scope('dense2'):\n",
    "        # dense\n",
    "        # relu\n",
    "        h_fc2 = tf.nn.relu(tf.matmul(h_fc1, weights['wd2']) + biases['bd2'])\n",
    "        # dropout\n",
    "    \n",
    "    with tf.name_scope('dense3'):\n",
    "        # dense with softmax\n",
    "        y_conv=tf.nn.softmax(tf.matmul(h_fc2, weights['out']) + biases['out'])\n",
    "\n",
    "\n",
    "    \n",
    "    return y_conv    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = conv_net(x, weights, biases)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 1.918591, Training Accuracy= 0.12500\n",
      "Iter 2560, Minibatch Loss= 1.895154, Training Accuracy= 0.14844\n",
      "Iter 3840, Minibatch Loss= 1.902966, Training Accuracy= 0.14062\n",
      "Iter 5120, Minibatch Loss= 1.809216, Training Accuracy= 0.23438\n",
      "Iter 6400, Minibatch Loss= 1.918591, Training Accuracy= 0.12500\n",
      "Iter 7680, Minibatch Loss= 1.918591, Training Accuracy= 0.12500\n",
      "Iter 8960, Minibatch Loss= 1.910779, Training Accuracy= 0.13281\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.171875\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    merged = tf.merge_all_summaries()\n",
    "    writer = tf.train.SummaryWriter(FLAGS.summaries_dir, sess.graph)\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        \n",
    "        # prepare the next batch\n",
    "        batch_x, batch_y = next_batch(batch_size)\n",
    "        \n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        \n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "        \n",
    "        sess.run(pred, \n",
    "                 feed_dict={x: batch_x, y: batch_y, keep_prob: dropout},\n",
    "                 options=run_options,\n",
    "                 run_metadata=run_metadata)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            \n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "            \n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "            \n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: X_test[:256], y: y_test[:256], keep_prob: 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
