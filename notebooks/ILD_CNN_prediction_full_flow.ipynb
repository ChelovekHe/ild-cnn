{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ILD_CNN_prediction_full_flow\n",
    "### May 22, 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using patient 121 who has fibrosis\n",
    "# as reference, patient 138 is healthy\n",
    "# patient 107 has both reticulation and groundglass\n",
    "\n",
    "patient_ID = 121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "from keras.utils.data_utils import get_file\n",
    "from six.moves import cPickle\n",
    "import sys\n",
    "import cPickle as pickle\n",
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils \n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D,AveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "sys.path.insert(0, '../python')\n",
    "import ild_helpers as H\n",
    "import cnn_model as CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/peterhirt/datascience/ild-cnn\n"
     ]
    }
   ],
   "source": [
    "one_folder_up = os.path.dirname(os.getcwd())\n",
    "print one_folder_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create path to patch directory\n",
    "predict_dir = os.path.join(one_folder_up, 'predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking out the item :  .DS_Store\n"
     ]
    }
   ],
   "source": [
    "# list all directories under patch directory. They are representing the categories\n",
    "patient_list = (os.listdir(predict_dir))\n",
    "print 'taking out the item : ', patient_list.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/peterhirt/datascience/ild-cnn/predict/121/patch_bmp'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create path to patient directory\n",
    "patient_dir = os.path.join(predict_dir, str(patient_ID))\n",
    "patient_dir = os.path.join(patient_dir, 'patch_bmp')\n",
    "patient_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all directories under patch directory. They are representing the categories\n",
    "image_files = (os.listdir(patient_dir))\n",
    "len(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling from this list\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['p_0001_160_256.bmp',\n",
       " 'p_0002_128_256.bmp',\n",
       " 'p_0002_128_288.bmp',\n",
       " 'p_0002_160_256.bmp',\n",
       " 'p_0002_160_288.bmp']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'sampling from this list'\n",
    "image_files[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset X shape is now:  (337, 32, 32)\n",
      "X_file_reference list for the first 5 items is : \n",
      "['p_0001_160_256.bmp']\n",
      "['p_0002_128_256.bmp']\n",
      "['p_0002_128_288.bmp']\n",
      "['p_0002_160_256.bmp']\n",
      "['p_0002_160_288.bmp']\n"
     ]
    }
   ],
   "source": [
    "# creating variables\n",
    "# list for the merged pixel data\n",
    "dataset_list = []\n",
    "# list of the file reference data\n",
    "file_reference_list = []\n",
    "\n",
    "\n",
    "# go through all image files\n",
    "# \n",
    "\n",
    "\n",
    "for file in image_files:\n",
    "                \n",
    "    if file.find('.bmp') > 0:\n",
    "                \n",
    "        # load the .bmp file into memory       \n",
    "        image = misc.imread(os.path.join(str(patient_dir),file), flatten= 0)\n",
    "        \n",
    "        # append the array to the dataset list\n",
    "        dataset_list.append(image)\n",
    "        \n",
    "        # append the file name to the reference list. The objective here is to ensure that the data \n",
    "        # and the file information about the x/y position is guamarteed\n",
    "        \n",
    "        file_reference_list.append(file)\n",
    "                \n",
    "                \n",
    "                                 \n",
    "# transform dataset list into numpy array                   \n",
    "dataset = np.array(dataset_list)\n",
    "file_reference = np.array(file_reference_list)\n",
    "\n",
    "# use only one of the 3 color channels as greyscale info\n",
    "X_predict = dataset[:,:, :]\n",
    "\n",
    "print 'dataset X shape is now: ', X_predict.shape\n",
    "print 'X_file_reference list for the first 5 items is : ' \n",
    "print file_reference[0:1]\n",
    "print file_reference[1:2]\n",
    "print file_reference[2:3]\n",
    "print file_reference[3:4]\n",
    "print file_reference[4:5]\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(337, 32, 32)\n",
      "(337,)\n"
     ]
    }
   ],
   "source": [
    "print X_predict.shape\n",
    "print file_reference.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args         = H.parse_args()                          \n",
    "train_params = {\n",
    "     'do' : float(args.do) if args.do else 0.5,        \n",
    "     'a'  : float(args.a) if args.a else 0.3,          # Conv Layers LeakyReLU alpha param [if alpha set to 0 LeakyReLU is equivalent with ReLU]\n",
    "     'k'  : int(args.k) if args.k else 4,              # Feature maps k multiplier\n",
    "     's'  : float(args.s) if args.s else 1,            # Input Image rescale factor\n",
    "     'pf' : float(args.pf) if args.pf else 1,          # Percentage of the pooling layer: [0,1]\n",
    "     'pt' : args.pt if args.pt else 'Avg',             # Pooling type: Avg, Max\n",
    "     'fp' : args.fp if args.fp else 'proportional',    # Feature maps policy: proportional, static\n",
    "     'cl' : int(args.cl) if args.cl else 5,            # Number of Convolutional Layers\n",
    "     'opt': args.opt if args.opt else 'Adam',          # Optimizer: SGD, Adagrad, Adam\n",
    "     'obj': args.obj if args.obj else 'ce',            # Minimization Objective: mse, ce\n",
    "     'patience' : args.pat if args.pat else 5,         # Patience parameter for early stoping\n",
    "     'tolerance': args.tol if args.tol else 1.005,     # Tolerance parameter for early stoping [default: 1.005, checks if > 0.5%]\n",
    "     'res_alias': args.csv if args.csv else 'res'      # csv results filename alias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load both the model and the weights \n",
    "\n",
    "model = H.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss=CNN.get_Obj(train_params['obj']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adding a singleton dimension and rescale to [0,1]\n",
    "\n",
    "X_predict = np.asarray(np.expand_dims(X_predict,1))/float(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 3s     \n",
      "337/337 [==============================] - 3s     \n"
     ]
    }
   ],
   "source": [
    "# predict and check classification and probabilities are the same\n",
    "\n",
    "classes = model.predict_classes(X_predict, batch_size=10)\n",
    "proba = model.predict_proba(X_predict, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 1, 3, 1, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 2,\n",
       "       3, 2, 1, 2, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1,\n",
       "       2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 1, 1, 1, 2, 3, 3,\n",
       "       3, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 1, 3, 1, 2, 2, 1,\n",
       "       2, 1, 3, 3, 3, 1, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1,\n",
       "       1, 3, 2, 2, 2, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3,\n",
       "       1, 3, 1, 3, 3, 2, 2, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3,\n",
       "       3, 3, 3, 1, 1, 1, 1, 3, 3, 1, 3, 3, 1, 2, 1, 2, 2, 2, 1, 3, 3, 3, 3,\n",
       "       3, 1, 1, 3, 3, 1, 1, 3, 3, 1, 3, 3, 1, 1, 3, 3, 3, 1, 3, 3, 1, 5, 2,\n",
       "       2, 2, 3, 3, 3, 1, 1, 3, 3, 1, 3, 1, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3,\n",
       "       1, 3, 1, 3, 3, 1, 2, 2, 0, 2, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3,\n",
       "       3, 3, 3, 1, 1, 0, 2, 1, 1, 3, 2, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1, 1,\n",
       "       3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 1, 1, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.81898398e-15,   1.00000000e+00,   8.96222797e-17,\n",
       "          9.47430120e-11,   2.56934585e-26,   3.32834584e-32],\n",
       "       [  1.71485317e-34,   2.06839788e-11,   4.55131852e-34,\n",
       "          1.00000000e+00,   9.44423814e-27,   0.00000000e+00],\n",
       "       [  1.08145679e-04,   9.97419953e-01,   2.35747057e-03,\n",
       "          1.14186594e-04,   9.98672789e-08,   1.51967058e-07],\n",
       "       ..., \n",
       "       [  6.99287266e-05,   8.97734940e-01,   9.52222869e-02,\n",
       "          6.97282236e-03,   1.68149661e-09,   3.01491249e-10],\n",
       "       [  3.06105889e-19,   1.00000000e+00,   1.29432689e-17,\n",
       "          6.54884513e-13,   1.68453691e-25,   4.15287943e-29],\n",
       "       [  2.65061886e-28,   4.00789588e-07,   9.43119863e-20,\n",
       "          9.99999583e-01,   6.23481208e-26,   2.64940362e-37]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pickle/predicted_classes_121.pkl\n",
      "../pickle/predicted_probabilities_121.pkl\n"
     ]
    }
   ],
   "source": [
    "# generate the pickl file name with the patient ID suffix\n",
    "\n",
    "file_name_classes = '../pickle/' + 'predicted_classes' + '_' + str(patient_ID) + '.pkl'\n",
    "file_name_probabilities = '../pickle/' +  'predicted_probabilities' + '_' + str(patient_ID) + '.pkl'\n",
    "print file_name_classes\n",
    "print file_name_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(classes, open( file_name_classes, \"wb\" ))\n",
    "pickle.dump(proba, open( file_name_probabilities, \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction completed\n"
     ]
    }
   ],
   "source": [
    "print 'prediction completed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
