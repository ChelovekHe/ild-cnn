{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "from keras.utils.data_utils import get_file\n",
    "from six.moves import cPickle\n",
    "import sys\n",
    "from keras.utils import np_utils \n",
    "import cPickle as pickle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define the working directory\n",
    "cwd=os.getcwd()\n",
    "(cwdtop,tail)=os.path.split(cwd)\n",
    "patch_dir=os.path.join(cwdtop,'patches')\n",
    "pickle_dir=os.path.join(cwdtop,'pickle')\n",
    "if not os.path.exists(pickle_dir):\n",
    "    os.mkdir(pickle_dir)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/peterhirt/datascience/ild-cnn/pickle\n"
     ]
    }
   ],
   "source": [
    "print pickle_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['consolidation', 'fibrosis', 'ground_glass', 'healthy', 'micronodules', 'reticulation']\n"
     ]
    }
   ],
   "source": [
    "# define a list of used labels\n",
    "usedclassif = ['consolidation',\n",
    " 'fibrosis',\n",
    " 'ground_glass',\n",
    " 'healthy',\n",
    " 'micronodules',\n",
    " 'reticulation']\n",
    "print (usedclassif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a dictionary with labels\n",
    "classif={}\n",
    "i=0\n",
    "for f in usedclassif:\n",
    "    classif[f]=i\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'micronodules': 4, 'healthy': 3, 'ground_glass': 2, 'consolidation': 0, 'reticulation': 5, 'fibrosis': 1}\n"
     ]
    }
   ],
   "source": [
    "print classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list all directories under patch directory. They are representing the categories\n",
    "\n",
    "category_list=os.walk( patch_dir).next()[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['consolidation',\n",
       " 'fibrosis',\n",
       " 'ground_glass',\n",
       " 'healthy',\n",
       " 'micronodules',\n",
       " 'reticulation']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print what we have as categories\n",
    "category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the path into the categories is:  /Users/peterhirt/datascience/ild-cnn/patches/consolidation\n",
      "the sub categories are :  ['apical', 'diffuse', 'non-relevant', 'peripheral_sub_pleural']\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/consolidation/apical\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/consolidation/diffuse\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/consolidation/non-relevant\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/consolidation/peripheral_sub_pleural\n",
      "the path into the categories is:  /Users/peterhirt/datascience/ild-cnn/patches/fibrosis\n",
      "the sub categories are :  ['apical', 'basal', 'diffuse', 'non-relevant', 'perihilar', 'peripheral_sub_pleural']\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/fibrosis/apical\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/fibrosis/basal\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/fibrosis/diffuse\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/fibrosis/non-relevant\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/fibrosis/perihilar\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/fibrosis/peripheral_sub_pleural\n",
      "the path into the categories is:  /Users/peterhirt/datascience/ild-cnn/patches/ground_glass\n",
      "the sub categories are :  ['apical', 'basal', 'diffuse', 'non-relevant', 'peripheral_sub_pleural']\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/ground_glass/apical\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/ground_glass/basal\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/ground_glass/diffuse\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/ground_glass/non-relevant\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/ground_glass/peripheral_sub_pleural\n",
      "the path into the categories is:  /Users/peterhirt/datascience/ild-cnn/patches/healthy\n",
      "the sub categories are :  ['apical', 'non-relevant']\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/healthy/apical\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/healthy/non-relevant\n",
      "the path into the categories is:  /Users/peterhirt/datascience/ild-cnn/patches/micronodules\n",
      "the sub categories are :  ['diffuse', 'non-relevant', 'peripheral_sub_pleural']\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/micronodules/diffuse\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/micronodules/non-relevant\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/micronodules/peripheral_sub_pleural\n",
      "the path into the categories is:  /Users/peterhirt/datascience/ild-cnn/patches/reticulation\n",
      "the sub categories are :  ['apical', 'basal', 'non-relevant', 'peripheral_sub_pleural']\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/reticulation/apical\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/reticulation/basal\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/reticulation/non-relevant\n",
      "the path into the sub categories is: \n",
      "/Users/peterhirt/datascience/ild-cnn/patches/reticulation/peripheral_sub_pleural\n"
     ]
    }
   ],
   "source": [
    "# creating variables\n",
    "# list for the merged pixel data\n",
    "dataset_list = []\n",
    "# list of the label data\n",
    "label_list = []\n",
    "\n",
    "\n",
    "\n",
    "# go through all categories\n",
    "# \n",
    "for category in category_list:\n",
    "    if category in usedclassif:\n",
    "        category_dir = os.path.join(patch_dir, category)\n",
    "        print  'the path into the categories is: ', category_dir\n",
    "\n",
    "        sub_categories_dir_list = (os.listdir(category_dir))\n",
    "        print 'the sub categories are : ', sub_categories_dir_list\n",
    "\n",
    "        for subCategory in sub_categories_dir_list:\n",
    "\n",
    "            subCategory_dir = os.path.join(category_dir, subCategory)\n",
    "            print  'the path into the sub categories is: '\n",
    "            print subCategory_dir\n",
    "\n",
    "            image_files = (os.listdir(subCategory_dir))\n",
    "\n",
    "            for file in image_files:\n",
    "\n",
    "                if file.find('.bmp') > 0:\n",
    "\n",
    "                    # load the .bmp file into array       \n",
    "                    image = misc.imread(os.path.join(subCategory_dir,file), flatten= 0)\n",
    "                    # append the array to the dataset list\n",
    "                    dataset_list.append(image)\n",
    "\n",
    "                    # created rotated copies of images\n",
    "                    image90 = np.rot90(image)\n",
    "                    dataset_list.append(image90)\n",
    "\n",
    "                    image180 = np.rot90(image90)\n",
    "                    dataset_list.append(image180)\n",
    "\n",
    "                    image270 = np.rot90(image180)\n",
    "                    dataset_list.append(image270)\n",
    "\n",
    "                    # append  lables \n",
    "                    label_list.append(classif[category])\n",
    "                    # append also lables for rotated images\n",
    "                    label_list.append(classif[category])\n",
    "                    label_list.append(classif[category])\n",
    "                    label_list.append(classif[category])\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25572, 25572)\n"
     ]
    }
   ],
   "source": [
    "print (len(dataset_list),len(label_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape is now:  (25000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "nb_elem = 25000 \n",
    "indices = []  \n",
    "resultatx=[]\n",
    "resultaty= [] \n",
    "while nb_elem > 0:  \n",
    "    i = random.randint(0, len(dataset_list) -1)  \n",
    "    while i in indices: # tant que le tirage redonne un nombre déjà choisi  \n",
    "        i = random.randint(0, len(dataset_list) -1)  \n",
    "    indices.append(i)  \n",
    "    nb_elem = nb_elem - 1  \n",
    "\n",
    "for index in indices:  \n",
    "    resultatx.append(dataset_list[index])  \n",
    "    resultaty.append(label_list[index]) \n",
    "\n",
    "# transform dataset list into numpy array                   \n",
    "dataset = np.array(resultatx)\n",
    "\n",
    "# this is already in greyscale\n",
    "# use only one of the 3 color channels as greyscale info\n",
    "X = dataset[:,:,:]\n",
    "\n",
    "print 'dataset shape is now: ', X.shape\n",
    "\n",
    "# \n",
    "y = np.array(resultaty)\n",
    "# sampling item 22\n",
    "#y[22]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25572, 25572)\n",
      "(25000, 25000)\n"
     ]
    }
   ],
   "source": [
    "print (len(dataset_list),len(label_list))\n",
    "print (len(resultatx),len(resultaty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 32, 32)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 32, 32)\n",
      "(6250, 32, 32)\n",
      "(6250, 32, 32)\n",
      "(12500,)\n",
      "(6250,)\n",
      "(6250,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_intermediate, y_train, y_intermediate = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_intermediate, y_intermediate, test_size=0.5, random_state=42)\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape\n",
    "print y_val.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the dataset and label set into serial formatted pkl \n",
    "\n",
    "pickle.dump(X_train, open( os.path.join(pickle_dir,\"X_train.pkl\"), \"wb\" ))\n",
    "pickle.dump(X_test, open( os.path.join(pickle_dir,\"X_test.pkl\"), \"wb\" ))\n",
    "pickle.dump(X_val, open(os.path.join(pickle_dir,\"X_val.pkl\"), \"wb\" ))\n",
    "pickle.dump(y_train, open( os.path.join(pickle_dir,\"y_train.pkl\"), \"wb\" ))\n",
    "pickle.dump(y_test, open( os.path.join(pickle_dir,\"y_test.pkl\"), \"wb\" ))\n",
    "pickle.dump(y_val, open( os.path.join(pickle_dir,\"y_val.pkl\"), \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing if pickls was working fine\n",
    "recuperated_X_train = pickle.load( open( os.path.join(pickle_dir,\"X_train.pkl\"), \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 14,  23,  28, ...,  49,  21,   0],\n",
       "        [ 21,  21,  26, ...,  30,  19,   0],\n",
       "        [ 16,  16,  37, ...,  37,  58, 119],\n",
       "        ..., \n",
       "        [ 19,  37,  75, ...,  30,  40,  49],\n",
       "        [ 33,  37,  51, ...,  49,  49,  96],\n",
       "        [ 30,  33,  40, ...,  73,  91,  91]],\n",
       "\n",
       "       [[ 71,  86,  95, ..., 154, 178, 190],\n",
       "        [ 62,  98, 122, ..., 148, 190, 213],\n",
       "        [ 86,  92,  98, ..., 166, 163, 178],\n",
       "        ..., \n",
       "        [ 92,  83, 113, ...,  44,  50,  62],\n",
       "        [104, 101,  86, ...,  42,  50,  68],\n",
       "        [ 92,  95,  56, ...,  68,  59,  59]],\n",
       "\n",
       "       [[163, 157, 113, ..., 130, 113,  86],\n",
       "        [102, 121,  98, ..., 134, 121, 115],\n",
       "        [105, 150, 130, ...,  96, 121, 115],\n",
       "        ..., \n",
       "        [176, 161, 151, ..., 107,  94,  81],\n",
       "        [159, 171, 167, ...,  81,  63,  71],\n",
       "        [125, 165, 180, ...,  77,  86,  92]],\n",
       "\n",
       "       ..., \n",
       "       [[ 82, 105,  64, ...,  75, 101, 109],\n",
       "        [ 86, 109,  94, ..., 131, 135,  90],\n",
       "        [105, 131, 142, ..., 120, 120,  49],\n",
       "        ..., \n",
       "        [ 45, 105,  86, ...,   0,   0,   0],\n",
       "        [ 60,  97,  75, ...,   0,   0,   0],\n",
       "        [ 49,  49,  34, ...,   0,   0,   0]],\n",
       "\n",
       "       [[ 35,  32,  51, ..., 178,  53,   7],\n",
       "        [  7,  25,  19, ..., 158,  81,  21],\n",
       "        [  5,  16,  14, ..., 148, 116,  56],\n",
       "        ..., \n",
       "        [118,  97,  60, ...,  16,  28,  21],\n",
       "        [111,  93,  60, ...,   0,   7,  32],\n",
       "        [127,  70,  35, ...,  16,  21,  49]],\n",
       "\n",
       "       [[ 85,  85,  78, ...,  92, 111, 131],\n",
       "        [ 98,  92,  98, ..., 111, 118, 124],\n",
       "        [111, 105, 105, ..., 111,  98,  98],\n",
       "        ..., \n",
       "        [ 59,  85, 118, ...,  65,  92, 150],\n",
       "        [ 72,  85,  85, ...,  72, 111, 137],\n",
       "        [ 65,  72,  72, ...,  85, 163, 163]]], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recuperated_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
